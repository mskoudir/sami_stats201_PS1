# -*- coding: utf-8 -*-
"""Copy of PS1 STATS201.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfIcJ2S_aeWnWbgDSazvshyFuzyc7K3y

1️⃣ Install & Import Required Libraries
"""

# Install required packages (Run this only if needed)
!pip install shap tensorflow scikit-learn pandas numpy matplotlib seaborn

# Import necessary libraries
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.inspection import permutation_importance
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Ensure SHAP is initialized for visualization
shap.initjs()

"""2️⃣ Load and Preprocess Dataset"""

# Load dataset
file_path = "220831_STATE OF HIRING DISCRIMINATION.xlsx"
xls = pd.ExcelFile(file_path)

# Load relevant sheets
df = xls.parse('register')

# Select necessary columns
columns_to_keep = [
    "ground", "study", "treatment_group_high", "control_group",
    "callback_maj", "callback_min", "effect_category"
]
df = df[columns_to_keep].dropna()

# Encode categorical variables
df["effect_category_encoded"] = df["effect_category"].astype("category").cat.codes
df["treatment_group_encoded"] = df["treatment_group_high"].astype("category").cat.codes
df["control_group_encoded"] = df["control_group"].astype("category").cat.codes

# Compute callback rate difference
df["callback_diff"] = df["callback_maj"] - df["callback_min"]

# Define features & target variable
X = df[["treatment_group_encoded", "control_group_encoded", "callback_diff"]]
y = df["effect_category_encoded"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Display dataset info
df.head()

"""3️⃣ Exploratory Data Analysis (EDA)"""

# Plot Callback Rates
plt.figure(figsize=(10, 5))
sns.barplot(x=df["treatment_group_high"], y=df["callback_diff"], palette="coolwarm")
plt.xlabel("Treatment Group")
plt.ylabel("Callback Rate Difference")
plt.title("Callback Rate Disparities by Treatment Group")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

"""4️⃣ Train Machine Learning Models

(a) Logistic Regression
"""

logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train_scaled, y_train)
y_pred_logistic = logistic_model.predict(X_test_scaled)

# Evaluate performance
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_logistic):.4f}")
print(classification_report(y_test, y_pred_logistic))

"""(b) Decision Tree & Random Forest"""

dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)

dt_model.fit(X_train_scaled, y_train)
rf_model.fit(X_train_scaled, y_train)

y_pred_dt = dt_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate performance
print(f"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}")
print(classification_report(y_test, y_pred_dt))

print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(classification_report(y_test, y_pred_rf))

"""5️⃣ Fairness Metrics Calculation"""

# Compute fairness metrics
demographic_parity = df.groupby("treatment_group_encoded")["callback_diff"].mean()
disparate_impact_ratio = df["callback_min"].mean() / df["callback_maj"].mean()

# Compute confusion matrices for equalized odds
cm_rf = confusion_matrix(y_test, y_pred_rf)
cm_dt = confusion_matrix(y_test, y_pred_dt)

tpr_rf = cm_rf[1,1] / (cm_rf[1,1] + cm_rf[1,0]) if (cm_rf[1,1] + cm_rf[1,0]) > 0 else 0
fpr_rf = cm_rf[0,1] / (cm_rf[0,1] + cm_rf[0,0]) if (cm_rf[0,1] + cm_rf[0,0]) > 0 else 0

# Display fairness metrics
print(f"Demographic Parity: {demographic_parity}")
print(f"Disparate Impact Ratio: {disparate_impact_ratio:.4f}")
print(f"Equalized Odds - True Positive Rate (Random Forest): {tpr_rf:.4f}")
print(f"Equalized Odds - False Positive Rate (Random Forest): {fpr_rf:.4f}")

"""6️⃣ Feature Importance Using SHAP"""

# Use SHAP for feature importance visualization
explainer = shap.Explainer(rf_model.predict, X_train_scaled)
shap_values = explainer(X_test_scaled)

# SHAP Summary Plot
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns)

"""7️⃣ Fairness Metrics Visualization"""

# Create a bar plot for demographic parity
plt.figure(figsize=(10, 5))
sns.barplot(x=demographic_parity.index, y=demographic_parity.values, palette="coolwarm")
plt.xlabel("Treatment Group (Encoded)")
plt.ylabel("Average Callback Rate")
plt.title("Demographic Parity: Average Callback Rate per Group")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# Plot Disparate Impact Ratio
plt.figure(figsize=(6, 4))
plt.bar(["Disparate Impact Ratio"], [disparate_impact_ratio], color="steelblue")
plt.axhline(y=0.8, color="red", linestyle="--", label="Adverse Impact Threshold (0.8)")
plt.ylabel("Ratio")
plt.title("Disparate Impact Ratio")
plt.legend()
plt.ylim(0, 1.2)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

""" Train Neural Network"""

nn_model = Sequential([
    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(8, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')
])

nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = nn_model.fit(X_train_scaled, y_train, epochs=50, batch_size=8, validation_data=(X_test_scaled, y_test), verbose=1)

# Evaluate Neural Network
nn_loss, nn_accuracy = nn_model.evaluate(X_test_scaled, y_test)
print(f"Neural Network Accuracy: {nn_accuracy:.4f}")

"""Feature Importance Using SHAP"""

# Use KernelExplainer for SHAP
explainer_nn = shap.Explainer(nn_model, X_train_scaled[:100])  # Use a subset for efficiency
shap_values_nn = explainer_nn(X_test_scaled[:100])

# Generate SHAP summary plot
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_nn, X_test_scaled[:100], feature_names=X.columns)
